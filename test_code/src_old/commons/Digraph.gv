digraph {
	graph [size="30.9,30.9"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139908899049776 [label="
 ()" fillcolor=darkolivegreen1]
	139908899360096 [label="AddBackward0
------------
alpha: 1"]
	139908899360384 -> 139908899360096
	139908899360384 [label="MseLossBackward0
-------------------------
reduction:              1
self     : [saved tensor]
target   : [saved tensor]"]
	139908899360048 -> 139908899360384
	139908899360048 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	139908899360480 -> 139908899360048
	139908899360480 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :      (10, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :       (512, 6)
mat2_strides:       (1, 512)"]
	139908899360576 -> 139908899360480
	139908898944160 [label="model_b.actor.2.bias
 (6)" fillcolor=lightblue]
	139908898944160 -> 139908899360576
	139908899360576 [label=AccumulateGrad]
	139908899360528 -> 139908899360480
	139908899360528 [label="AddBackward0
------------
alpha: 1"]
	139908899360672 -> 139908899360528
	139908899360672 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899074256 -> 139908899360672
	139908899074256 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (10, 3136)
mat1_strides:      (3136, 1)
mat2        : [saved tensor]
mat2_sizes  :    (3136, 512)
mat2_strides:      (1, 3136)"]
	139908899074352 -> 139908899074256
	139908898944000 [label="model_b.actor.0.bias
 (512)" fillcolor=lightblue]
	139908898944000 -> 139908899074352
	139908899074352 [label=AccumulateGrad]
	139908899074304 -> 139908899074256
	139908899074304 [label="ViewBackward0
--------------------------
self_sizes: (10, 64, 7, 7)"]
	139908899074448 -> 139908899074304
	139908899074448 [label="AddBackward0
------------
alpha: 1"]
	139908899074640 -> 139908899074448
	139908899074640 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899074784 -> 139908899074640
	139908899074784 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	139908899074880 -> 139908899074784
	139908899074880 [label="AddBackward0
------------
alpha: 1"]
	139908899075072 -> 139908899074880
	139908899075072 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899075216 -> 139908899075072
	139908899075216 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	139908899075312 -> 139908899075216
	139908899075312 [label="AddBackward0
------------
alpha: 1"]
	139908899075504 -> 139908899075312
	139908899075504 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899075648 -> 139908899075504
	139908899075648 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (32,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (4, 4)
transposed    :          False
weight        : [saved tensor]"]
	139908899075744 -> 139908899075648
	139908899307408 [label="model_b.layer1.0.weight
 (32, 1, 8, 8)" fillcolor=lightblue]
	139908899307408 -> 139908899075744
	139908899075744 [label=AccumulateGrad]
	139908899075696 -> 139908899075648
	139908898943040 [label="model_b.layer1.0.bias
 (32)" fillcolor=lightblue]
	139908898943040 -> 139908899075696
	139908899075696 [label=AccumulateGrad]
	139908899075456 -> 139908899075312
	139908899075456 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (32,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	139908899075792 -> 139908899075456
	139908899075792 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899076032 -> 139908899075792
	139908899076032 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (32,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (4, 4)
transposed    :          False
weight        : [saved tensor]"]
	139908899076128 -> 139908899076032
	139908899306048 [label="model_a.layer1.0.weight
 (32, 1, 8, 8)" fillcolor=lightblue]
	139908899306048 -> 139908899076128
	139908899076128 [label=AccumulateGrad]
	139908899076080 -> 139908899076032
	139908899306128 [label="model_a.layer1.0.bias
 (32)" fillcolor=lightblue]
	139908899306128 -> 139908899076080
	139908899076080 [label=AccumulateGrad]
	139908899075552 -> 139908899075456
	139908898944320 [label="model_b.adaptor.conv1_adaptor.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139908898944320 -> 139908899075552
	139908899075552 [label=AccumulateGrad]
	139908899075600 -> 139908899075456
	139908898944400 [label="model_b.adaptor.conv1_adaptor.bias
 (32)" fillcolor=lightblue]
	139908898944400 -> 139908899075600
	139908899075600 [label=AccumulateGrad]
	139908899075264 -> 139908899075216
	139908898943200 [label="model_b.layer2.0.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	139908898943200 -> 139908899075264
	139908899075264 [label=AccumulateGrad]
	139908899075120 -> 139908899075216
	139908898943280 [label="model_b.layer2.0.bias
 (64)" fillcolor=lightblue]
	139908898943280 -> 139908899075120
	139908899075120 [label=AccumulateGrad]
	139908899075024 -> 139908899074880
	139908899075024 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	139908899075408 -> 139908899075024
	139908899075408 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899076176 -> 139908899075408
	139908899076176 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	139908899075792 -> 139908899076176
	139908899076272 -> 139908899076176
	139908899306288 [label="model_a.layer2.0.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	139908899306288 -> 139908899076272
	139908899076272 [label=AccumulateGrad]
	139908899076224 -> 139908899076176
	139908899306368 [label="model_a.layer2.0.bias
 (64)" fillcolor=lightblue]
	139908899306368 -> 139908899076224
	139908899076224 [label=AccumulateGrad]
	139908899075360 -> 139908899075024
	139908898944560 [label="model_b.adaptor.conv2_adaptor.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139908898944560 -> 139908899075360
	139908899075360 [label=AccumulateGrad]
	139908899075168 -> 139908899075024
	139908898944640 [label="model_b.adaptor.conv2_adaptor.bias
 (64)" fillcolor=lightblue]
	139908898944640 -> 139908899075168
	139908899075168 [label=AccumulateGrad]
	139908899074832 -> 139908899074784
	139908898943440 [label="model_b.layer3.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139908898943440 -> 139908899074832
	139908899074832 [label=AccumulateGrad]
	139908899074688 -> 139908899074784
	139908898943520 [label="model_b.layer3.0.bias
 (64)" fillcolor=lightblue]
	139908898943520 -> 139908899074688
	139908899074688 [label=AccumulateGrad]
	139908899074592 -> 139908899074448
	139908899074592 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	139908899074976 -> 139908899074592
	139908899074976 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899076320 -> 139908899074976
	139908899076320 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:          (64,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	139908899075408 -> 139908899076320
	139908899076416 -> 139908899076320
	139908899306528 [label="model_a.layer3.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139908899306528 -> 139908899076416
	139908899076416 [label=AccumulateGrad]
	139908899076368 -> 139908899076320
	139908899306608 [label="model_a.layer3.0.bias
 (64)" fillcolor=lightblue]
	139908899306608 -> 139908899076368
	139908899076368 [label=AccumulateGrad]
	139908899074928 -> 139908899074592
	139908898944800 [label="model_b.adaptor.conv3_adaptor.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139908898944800 -> 139908899074928
	139908899074928 [label=AccumulateGrad]
	139908899074736 -> 139908899074592
	139908898944880 [label="model_b.adaptor.conv3_adaptor.bias
 (64)" fillcolor=lightblue]
	139908898944880 -> 139908899074736
	139908899074736 [label=AccumulateGrad]
	139908899074160 -> 139908899074256
	139908899074160 [label=TBackward0]
	139908899075888 -> 139908899074160
	139908898943680 [label="model_b.actor.0.weight
 (512, 3136)" fillcolor=lightblue]
	139908898943680 -> 139908899075888
	139908899075888 [label=AccumulateGrad]
	139908899360720 -> 139908899360528
	139908899360720 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (10, 3136)
mat1_strides:      (3136, 1)
mat2        : [saved tensor]
mat2_sizes  :    (3136, 512)
mat2_strides:      (1, 3136)"]
	139908899074544 -> 139908899360720
	139908898945200 [label="model_b.adaptor.actor_adaptor.bias
 (512)" fillcolor=lightblue]
	139908898945200 -> 139908899074544
	139908899074544 [label=AccumulateGrad]
	139908899074400 -> 139908899360720
	139908899074400 [label="ViewBackward0
--------------------------
self_sizes: (10, 64, 7, 7)"]
	139908899074976 -> 139908899074400
	139908899074208 -> 139908899360720
	139908899074208 [label=TBackward0]
	139908899076464 -> 139908899074208
	139908898945120 [label="model_b.adaptor.actor_adaptor.weight
 (512, 3136)" fillcolor=lightblue]
	139908898945120 -> 139908899076464
	139908899076464 [label=AccumulateGrad]
	139908899360432 -> 139908899360480
	139908899360432 [label=TBackward0]
	139908899074496 -> 139908899360432
	139908898944080 [label="model_b.actor.2.weight
 (6, 512)" fillcolor=lightblue]
	139908898944080 -> 139908899074496
	139908899074496 [label=AccumulateGrad]
	139908899360288 -> 139908899360096
	139908899360288 [label="MseLossBackward0
-------------------------
reduction:              1
self     : [saved tensor]
target   : [saved tensor]"]
	139908899360624 -> 139908899360288
	139908899360624 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :      (10, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :       (512, 1)
mat2_strides:       (1, 512)"]
	139908899360240 -> 139908899360624
	139908898943920 [label="model_b.critic.2.bias
 (1)" fillcolor=lightblue]
	139908898943920 -> 139908899360240
	139908899360240 [label=AccumulateGrad]
	139908899076512 -> 139908899360624
	139908899076512 [label="AddBackward0
------------
alpha: 1"]
	139908899074112 -> 139908899076512
	139908899074112 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	139908899076752 -> 139908899074112
	139908899076752 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (10, 3136)
mat1_strides:      (3136, 1)
mat2        : [saved tensor]
mat2_sizes  :    (3136, 512)
mat2_strides:      (1, 3136)"]
	139908899076848 -> 139908899076752
	139908898943760 [label="model_b.critic.0.bias
 (512)" fillcolor=lightblue]
	139908898943760 -> 139908899076848
	139908899076848 [label=AccumulateGrad]
	139908899074304 -> 139908899076752
	139908899076800 -> 139908899076752
	139908899076800 [label=TBackward0]
	139908899076896 -> 139908899076800
	139908898943360 [label="model_b.critic.0.weight
 (512, 3136)" fillcolor=lightblue]
	139908898943360 -> 139908899076896
	139908899076896 [label=AccumulateGrad]
	139908899075984 -> 139908899076512
	139908899075984 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (10, 3136)
mat1_strides:      (3136, 1)
mat2        : [saved tensor]
mat2_sizes  :    (3136, 512)
mat2_strides:      (1, 3136)"]
	139908899076944 -> 139908899075984
	139908898945040 [label="model_b.adaptor.critic_adaptor.bias
 (512)" fillcolor=lightblue]
	139908898945040 -> 139908899076944
	139908899076944 [label=AccumulateGrad]
	139908899074400 -> 139908899075984
	139908899076656 -> 139908899075984
	139908899076656 [label=TBackward0]
	139908899077088 -> 139908899076656
	139908898944960 [label="model_b.adaptor.critic_adaptor.weight
 (512, 3136)" fillcolor=lightblue]
	139908898944960 -> 139908899077088
	139908899077088 [label=AccumulateGrad]
	139908899075936 -> 139908899360624
	139908899075936 [label=TBackward0]
	139908899077040 -> 139908899075936
	139908898943840 [label="model_b.critic.2.weight
 (1, 512)" fillcolor=lightblue]
	139908898943840 -> 139908899077040
	139908899077040 [label=AccumulateGrad]
	139908899360096 -> 139908899049776
}

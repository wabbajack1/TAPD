{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de048296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython import display\n",
    "from gym import wrappers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06a7d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GAMMA = 0.9\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        return x \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if state.__class__ == ().__class__:\n",
    "            state = torch.from_numpy(state[0]).float().unsqueeze(0)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(Variable(state))\n",
    "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
    "        return highest_prob_action, log_prob\n",
    "\n",
    "\n",
    "def update_policy(policy_network, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "        \n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
    "\n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    \n",
    "    policy_network.optimizer.zero_grad()\n",
    "    policy_gradient = torch.stack(policy_gradient).sum()\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98f67f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gym(x):\n",
    "    plt.imshow(x)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a1a2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v0', render_mode=\"rgb_array\")\n",
    "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 128)\n",
    "    \n",
    "    max_episode_num = 5000\n",
    "    max_steps = 1000\n",
    "    numsteps = []\n",
    "    avg_numsteps = []\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(max_episode_num):\n",
    "        state = env.reset()\n",
    "        #img = plt.imshow(env.render())\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        for steps in range(max_steps):\n",
    "            #img.set_data(env.render())\n",
    "            #display.display(plt.gcf())\n",
    "            #display.clear_output(wait=True)\n",
    "            \n",
    "            action, log_prob = policy_net.get_action(state)\n",
    "            new_state, reward, done, _, _= env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                update_policy(policy_net, rewards, log_probs)\n",
    "                numsteps.append(steps)\n",
    "                avg_numsteps.append(np.mean(numsteps[-10:]))\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                if episode % 1 == 0:\n",
    "                    sys.stdout.write(\"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "    \n",
    "    plt.plot(numsteps)\n",
    "    plt.plot(avg_numsteps)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71c16e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KerimErekmen/Desktop/Praesentation/Studium/Semester7/BA/code_agnostic_rl/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0513,  1.0221,  0.9898,  0.9538,  0.9139,  0.8695,  0.8202,  0.7654,\n",
      "         0.7045,  0.6369,  0.5617,  0.4782,  0.3854,  0.2823,  0.1678,  0.0405,\n",
      "        -0.1010, -0.2581, -0.4327, -0.6267, -0.8423, -1.0818, -1.3479, -1.6436,\n",
      "        -1.9721, -2.3372])\n",
      "tensor(-7.5652e-08)\n",
      "episode: 0, total reward: 26.0, average_reward: 26.0, length: 25\n",
      "tensor([ 1.2154,  1.1237,  1.0218,  0.9087,  0.7829,  0.6432,  0.4879,  0.3154,\n",
      "         0.1238, -0.0892, -0.3258, -0.5887, -0.8809, -1.2054, -1.5661, -1.9668])\n",
      "tensor(1.1176e-07)\n",
      "episode: 1, total reward: 16.0, average_reward: 21.0, length: 15\n",
      "tensor([ 1.1998,  1.1188,  1.0287,  0.9287,  0.8175,  0.6940,  0.5568,  0.4043,\n",
      "         0.2348,  0.0466, -0.1626, -0.3950, -0.6532, -0.9402, -1.2590, -1.6132,\n",
      "        -2.0068])\n",
      "tensor(4.9086e-08)\n",
      "episode: 2, total reward: 17.0, average_reward: 19.667, length: 16\n",
      "tensor([ 1.0039,  0.9827,  0.9593,  0.9332,  0.9042,  0.8720,  0.8362,  0.7964,\n",
      "         0.7522,  0.7032,  0.6486,  0.5880,  0.5207,  0.4459,  0.3627,  0.2704,\n",
      "         0.1677,  0.0537, -0.0730, -0.2138, -0.3702, -0.5440, -0.7372, -0.9517,\n",
      "        -1.1901, -1.4551, -1.7494, -2.0765, -2.4398])\n",
      "tensor(5.7549e-08)\n",
      "episode: 3, total reward: 29.0, average_reward: 22.0, length: 28\n",
      "tensor([ 1.2768,  1.0970,  0.8972,  0.6753,  0.4286,  0.1546, -0.1499, -0.4882,\n",
      "        -0.8640, -1.2817, -1.7458])\n",
      "tensor(1.8694e-07)\n",
      "episode: 4, total reward: 11.0, average_reward: 19.8, length: 10\n",
      "tensor([ 1.0675,  1.0351,  0.9990,  0.9589,  0.9144,  0.8649,  0.8100,  0.7489,\n",
      "         0.6811,  0.6057,  0.5219,  0.4288,  0.3254,  0.2105,  0.0828, -0.0591,\n",
      "        -0.2167, -0.3919, -0.5865, -0.8027, -1.0430, -1.3099, -1.6066, -1.9361,\n",
      "        -2.3023])\n",
      "tensor(-8.1062e-08)\n",
      "episode: 5, total reward: 25.0, average_reward: 20.667, length: 24\n",
      "tensor([ 1.1340,  1.0833,  1.0271,  0.9646,  0.8951,  0.8179,  0.7322,  0.6369,\n",
      "         0.5310,  0.4134,  0.2827,  0.1374, -0.0239, -0.2032, -0.4024, -0.6238,\n",
      "        -0.8697, -1.1430, -1.4466, -1.7840, -2.1588])\n",
      "tensor(-1.8023e-07)\n",
      "episode: 6, total reward: 21.0, average_reward: 20.714, length: 20\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 7, total reward: 15.0, average_reward: 20.0, length: 14\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 8, total reward: 19.0, average_reward: 19.889, length: 18\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 9, total reward: 18.0, average_reward: 19.7, length: 17\n",
      "tensor([ 1.1998,  1.1188,  1.0287,  0.9287,  0.8175,  0.6940,  0.5568,  0.4043,\n",
      "         0.2348,  0.0466, -0.1626, -0.3950, -0.6532, -0.9402, -1.2590, -1.6132,\n",
      "        -2.0068])\n",
      "tensor(4.9086e-08)\n",
      "episode: 10, total reward: 17.0, average_reward: 18.8, length: 16\n",
      "tensor([ 1.1340,  1.0833,  1.0271,  0.9646,  0.8951,  0.8179,  0.7322,  0.6369,\n",
      "         0.5310,  0.4134,  0.2827,  0.1374, -0.0239, -0.2032, -0.4024, -0.6238,\n",
      "        -0.8697, -1.1430, -1.4466, -1.7840, -2.1588])\n",
      "tensor(-1.8023e-07)\n",
      "episode: 11, total reward: 21.0, average_reward: 19.3, length: 20\n",
      "tensor([ 0.9167,  0.9054,  0.8928,  0.8789,  0.8634,  0.8462,  0.8270,  0.8058,\n",
      "         0.7822,  0.7559,  0.7268,  0.6944,  0.6584,  0.6184,  0.5739,  0.5245,\n",
      "         0.4697,  0.4087,  0.3409,  0.2657,  0.1820,  0.0891, -0.0141, -0.1289,\n",
      "        -0.2563, -0.3980, -0.5553, -0.7302, -0.9245, -1.1403, -1.3802, -1.6467,\n",
      "        -1.9428, -2.2719, -2.6374])\n",
      "tensor(8.5149e-08)\n",
      "episode: 12, total reward: 35.0, average_reward: 21.1, length: 34\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 13, total reward: 18.0, average_reward: 20.0, length: 17\n",
      "tensor([ 0.5249,  0.5249,  0.5248,  0.5247,  0.5247,  0.5246,  0.5245,  0.5244,\n",
      "         0.5242,  0.5241,  0.5240,  0.5238,  0.5236,  0.5234,  0.5232,  0.5229,\n",
      "         0.5226,  0.5223,  0.5219,  0.5215,  0.5211,  0.5206,  0.5201,  0.5195,\n",
      "         0.5188,  0.5181,  0.5173,  0.5163,  0.5153,  0.5142,  0.5130,  0.5116,\n",
      "         0.5100,  0.5083,  0.5064,  0.5043,  0.5019,  0.4993,  0.4964,  0.4932,\n",
      "         0.4896,  0.4856,  0.4811,  0.4762,  0.4707,  0.4647,  0.4579,  0.4504,\n",
      "         0.4420,  0.4328,  0.4225,  0.4110,  0.3983,  0.3842,  0.3685,  0.3510,\n",
      "         0.3317,  0.3101,  0.2862,  0.2596,  0.2301,  0.1972,  0.1608,  0.1203,\n",
      "         0.0752,  0.0252, -0.0304, -0.0921, -0.1608, -0.2370, -0.3217, -0.4159,\n",
      "        -0.5205, -0.6367, -0.7658, -0.9093, -1.0687, -1.2459, -1.4427, -1.6614,\n",
      "        -1.9043, -2.1743, -2.4743, -2.8076, -3.1779, -3.5894, -4.0467])\n",
      "tensor(2.5623e-07)\n",
      "episode: 14, total reward: 87.0, average_reward: 27.6, length: 86\n",
      "tensor([ 1.1998,  1.1188,  1.0287,  0.9287,  0.8175,  0.6940,  0.5568,  0.4043,\n",
      "         0.2348,  0.0466, -0.1626, -0.3950, -0.6532, -0.9402, -1.2590, -1.6132,\n",
      "        -2.0068])\n",
      "tensor(4.9086e-08)\n",
      "episode: 15, total reward: 17.0, average_reward: 26.8, length: 16\n",
      "tensor([ 1.0039,  0.9827,  0.9593,  0.9332,  0.9042,  0.8720,  0.8362,  0.7964,\n",
      "         0.7522,  0.7032,  0.6486,  0.5880,  0.5207,  0.4459,  0.3627,  0.2704,\n",
      "         0.1677,  0.0537, -0.0730, -0.2138, -0.3702, -0.5440, -0.7372, -0.9517,\n",
      "        -1.1901, -1.4551, -1.7494, -2.0765, -2.4398])\n",
      "tensor(5.7549e-08)\n",
      "episode: 16, total reward: 29.0, average_reward: 27.6, length: 28\n",
      "tensor([ 0.7856,  0.7819,  0.7778,  0.7733,  0.7682,  0.7626,  0.7564,  0.7495,\n",
      "         0.7418,  0.7332,  0.7237,  0.7132,  0.7014,  0.6884,  0.6739,  0.6578,\n",
      "         0.6399,  0.6201,  0.5980,  0.5735,  0.5462,  0.5159,  0.4823,  0.4449,\n",
      "         0.4034,  0.3572,  0.3060,  0.2490,  0.1857,  0.1153,  0.0372, -0.0497,\n",
      "        -0.1462, -0.2534, -0.3725, -0.5048, -0.6519, -0.8153, -0.9969, -1.1986,\n",
      "        -1.4228, -1.6718, -1.9485, -2.2560, -2.5977, -2.9773])\n",
      "tensor(2.4490e-07)\n",
      "episode: 17, total reward: 46.0, average_reward: 30.7, length: 45\n",
      "tensor([ 1.0513,  1.0221,  0.9898,  0.9538,  0.9139,  0.8695,  0.8202,  0.7654,\n",
      "         0.7045,  0.6369,  0.5617,  0.4782,  0.3854,  0.2823,  0.1678,  0.0405,\n",
      "        -0.1010, -0.2581, -0.4327, -0.6267, -0.8423, -1.0818, -1.3479, -1.6436,\n",
      "        -1.9721, -2.3372])\n",
      "tensor(-7.5652e-08)\n",
      "episode: 18, total reward: 26.0, average_reward: 31.4, length: 25\n",
      "tensor([ 1.2768,  1.0970,  0.8972,  0.6753,  0.4286,  0.1546, -0.1499, -0.4882,\n",
      "        -0.8640, -1.2817, -1.7458])\n",
      "tensor(1.8694e-07)\n",
      "episode: 19, total reward: 11.0, average_reward: 30.7, length: 10\n",
      "tensor([ 0.6874,  0.6862,  0.6848,  0.6833,  0.6816,  0.6798,  0.6777,  0.6754,\n",
      "         0.6729,  0.6701,  0.6669,  0.6635,  0.6596,  0.6553,  0.6505,  0.6452,\n",
      "         0.6393,  0.6328,  0.6255,  0.6174,  0.6084,  0.5984,  0.5873,  0.5750,\n",
      "         0.5613,  0.5461,  0.5291,  0.5104,  0.4895,  0.4663,  0.4405,  0.4118,\n",
      "         0.3800,  0.3447,  0.3054,  0.2617,  0.2132,  0.1593,  0.0994,  0.0329,\n",
      "        -0.0411, -0.1232, -0.2145, -0.3159, -0.4286, -0.5538, -0.6930, -0.8476,\n",
      "        -1.0193, -1.2102, -1.4222, -1.6578, -1.9196, -2.2105, -2.5337, -2.8928,\n",
      "        -3.2918])\n",
      "tensor(-7.3199e-08)\n",
      "episode: 20, total reward: 57.0, average_reward: 34.7, length: 56\n",
      "tensor([ 0.9736,  0.9565,  0.9375,  0.9163,  0.8928,  0.8668,  0.8378,  0.8056,\n",
      "         0.7698,  0.7300,  0.6858,  0.6367,  0.5822,  0.5216,  0.4542,  0.3794,\n",
      "         0.2962,  0.2038,  0.1012, -0.0129, -0.1396, -0.2804, -0.4368, -0.6107,\n",
      "        -0.8038, -1.0184, -1.2569, -1.5218, -1.8162, -2.1433, -2.5068])\n",
      "tensor(-1.1536e-08)\n",
      "episode: 21, total reward: 31.0, average_reward: 35.7, length: 30\n",
      "tensor([ 0.9589,  0.9435,  0.9264,  0.9073,  0.8862,  0.8627,  0.8366,  0.8075,\n",
      "         0.7753,  0.7395,  0.6997,  0.6555,  0.6063,  0.5517,  0.4911,  0.4237,\n",
      "         0.3488,  0.2655,  0.1731,  0.0704, -0.0438, -0.1706, -0.3116, -0.4681,\n",
      "        -0.6421, -0.8354, -1.0502, -1.2889, -1.5541, -1.8487, -2.1761, -2.5398])\n",
      "tensor(1.5274e-07)\n",
      "episode: 22, total reward: 32.0, average_reward: 35.4, length: 31\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 23, total reward: 19.0, average_reward: 35.5, length: 18\n",
      "tensor([ 0.7465,  0.7440,  0.7413,  0.7383,  0.7349,  0.7312,  0.7270,  0.7224,\n",
      "         0.7172,  0.7115,  0.7052,  0.6981,  0.6903,  0.6816,  0.6719,  0.6612,\n",
      "         0.6492,  0.6360,  0.6212,  0.6049,  0.5867,  0.5664,  0.5440,  0.5190,\n",
      "         0.4913,  0.4605,  0.4262,  0.3882,  0.3459,  0.2990,  0.2468,  0.1888,\n",
      "         0.1244,  0.0528, -0.0267, -0.1151, -0.2133, -0.3224, -0.4436, -0.5783,\n",
      "        -0.7280, -0.8943, -1.0790, -1.2843, -1.5124, -1.7659, -2.0475, -2.3604,\n",
      "        -2.7081, -3.0944])\n",
      "tensor(-1.6570e-07)\n",
      "episode: 24, total reward: 50.0, average_reward: 31.8, length: 49\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 25, total reward: 18.0, average_reward: 31.9, length: 17\n",
      "tensor([ 1.0513,  1.0221,  0.9898,  0.9538,  0.9139,  0.8695,  0.8202,  0.7654,\n",
      "         0.7045,  0.6369,  0.5617,  0.4782,  0.3854,  0.2823,  0.1678,  0.0405,\n",
      "        -0.1010, -0.2581, -0.4327, -0.6267, -0.8423, -1.0818, -1.3479, -1.6436,\n",
      "        -1.9721, -2.3372])\n",
      "tensor(-7.5652e-08)\n",
      "episode: 26, total reward: 26.0, average_reward: 31.6, length: 25\n",
      "tensor([ 1.0513,  1.0221,  0.9898,  0.9538,  0.9139,  0.8695,  0.8202,  0.7654,\n",
      "         0.7045,  0.6369,  0.5617,  0.4782,  0.3854,  0.2823,  0.1678,  0.0405,\n",
      "        -0.1010, -0.2581, -0.4327, -0.6267, -0.8423, -1.0818, -1.3479, -1.6436,\n",
      "        -1.9721, -2.3372])\n",
      "tensor(-7.5652e-08)\n",
      "episode: 27, total reward: 26.0, average_reward: 29.6, length: 25\n",
      "tensor([ 1.2154,  1.1237,  1.0218,  0.9087,  0.7829,  0.6432,  0.4879,  0.3154,\n",
      "         0.1238, -0.0892, -0.3258, -0.5887, -0.8809, -1.2054, -1.5661, -1.9668])\n",
      "tensor(1.1176e-07)\n",
      "episode: 28, total reward: 16.0, average_reward: 28.6, length: 15\n",
      "tensor([ 0.9886,  0.9696,  0.9485,  0.9250,  0.8989,  0.8699,  0.8377,  0.8019,\n",
      "         0.7622,  0.7180,  0.6689,  0.6144,  0.5538,  0.4865,  0.4117,  0.3286,\n",
      "         0.2362,  0.1336,  0.0196, -0.1071, -0.2478, -0.4042, -0.5780, -0.7711,\n",
      "        -0.9856, -1.2240, -1.4889, -1.7831, -2.1101, -2.4735])\n",
      "tensor(2.0266e-07)\n",
      "episode: 29, total reward: 30.0, average_reward: 30.5, length: 29\n",
      "tensor([ 1.2443,  1.1258,  0.9942,  0.8480,  0.6855,  0.5050,  0.3044,  0.0815,\n",
      "        -0.1661, -0.4413, -0.7470, -1.0867, -1.4641, -1.8835])\n",
      "tensor(8.5149e-09)\n",
      "episode: 30, total reward: 14.0, average_reward: 26.2, length: 13\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 31, total reward: 19.0, average_reward: 25.0, length: 18\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 32, total reward: 18.0, average_reward: 23.6, length: 17\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 33, total reward: 15.0, average_reward: 23.2, length: 14\n",
      "tensor([ 1.0194,  0.9959,  0.9698,  0.9408,  0.9086,  0.8727,  0.8330,  0.7887,\n",
      "         0.7396,  0.6850,  0.6244,  0.5570,  0.4821,  0.3989,  0.3065,  0.2037,\n",
      "         0.0896, -0.0372, -0.1781, -0.3346, -0.5086, -0.7019, -0.9166, -1.1552,\n",
      "        -1.4204, -1.7149, -2.0423, -2.4059])\n",
      "tensor(1.8733e-07)\n",
      "episode: 34, total reward: 28.0, average_reward: 21.0, length: 27\n",
      "tensor([ 0.8068,  0.8023,  0.7973,  0.7917,  0.7855,  0.7786,  0.7710,  0.7625,\n",
      "         0.7531,  0.7426,  0.7310,  0.7181,  0.7037,  0.6877,  0.6700,  0.6503,\n",
      "         0.6284,  0.6041,  0.5771,  0.5470,  0.5137,  0.4766,  0.4354,  0.3896,\n",
      "         0.3388,  0.2823,  0.2195,  0.1497,  0.0722, -0.0139, -0.1096, -0.2160,\n",
      "        -0.3341, -0.4654, -0.6112, -0.7733, -0.9534, -1.1534, -1.3757, -1.6227,\n",
      "        -1.8972, -2.2021, -2.5410, -2.9175])\n",
      "tensor(-1.0092e-07)\n",
      "episode: 35, total reward: 44.0, average_reward: 23.6, length: 43\n",
      "tensor([ 0.9445,  0.9306,  0.9152,  0.8980,  0.8790,  0.8578,  0.8343,  0.8081,\n",
      "         0.7791,  0.7468,  0.7109,  0.6711,  0.6268,  0.5776,  0.5229,  0.4622,\n",
      "         0.3947,  0.3197,  0.2364,  0.1438,  0.0409, -0.0734, -0.2004, -0.3415,\n",
      "        -0.4983, -0.6725, -0.8660, -1.0811, -1.3201, -1.5856, -1.8806, -2.2084,\n",
      "        -2.5726])\n",
      "tensor(3.9736e-08)\n",
      "episode: 36, total reward: 33.0, average_reward: 24.3, length: 32\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 37, total reward: 15.0, average_reward: 23.2, length: 14\n",
      "tensor([ 1.2840,  1.0385,  0.7658,  0.4627,  0.1260, -0.2482, -0.6639, -1.1258,\n",
      "        -1.6391])\n",
      "tensor(-6.6227e-08)\n",
      "episode: 38, total reward: 9.0, average_reward: 22.5, length: 8\n",
      "tensor([ 1.0039,  0.9827,  0.9593,  0.9332,  0.9042,  0.8720,  0.8362,  0.7964,\n",
      "         0.7522,  0.7032,  0.6486,  0.5880,  0.5207,  0.4459,  0.3627,  0.2704,\n",
      "         0.1677,  0.0537, -0.0730, -0.2138, -0.3702, -0.5440, -0.7372, -0.9517,\n",
      "        -1.1901, -1.4551, -1.7494, -2.0765, -2.4398])\n",
      "tensor(5.7549e-08)\n",
      "episode: 39, total reward: 29.0, average_reward: 22.4, length: 28\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 40, total reward: 18.0, average_reward: 22.8, length: 17\n",
      "tensor([ 1.2825,  1.0733,  0.8409,  0.5826,  0.2956, -0.0232, -0.3775, -0.7711,\n",
      "        -1.2085, -1.6945])\n",
      "tensor(2.9802e-08)\n",
      "episode: 41, total reward: 10.0, average_reward: 21.9, length: 9\n",
      "tensor([ 0.9589,  0.9435,  0.9264,  0.9073,  0.8862,  0.8627,  0.8366,  0.8075,\n",
      "         0.7753,  0.7395,  0.6997,  0.6555,  0.6063,  0.5517,  0.4911,  0.4237,\n",
      "         0.3488,  0.2655,  0.1731,  0.0704, -0.0438, -0.1706, -0.3116, -0.4681,\n",
      "        -0.6421, -0.8354, -1.0502, -1.2889, -1.5541, -1.8487, -2.1761, -2.5398])\n",
      "tensor(1.5274e-07)\n",
      "episode: 42, total reward: 32.0, average_reward: 23.3, length: 31\n",
      "tensor([ 1.2443,  1.1258,  0.9942,  0.8480,  0.6855,  0.5050,  0.3044,  0.0815,\n",
      "        -0.1661, -0.4413, -0.7470, -1.0867, -1.4641, -1.8835])\n",
      "tensor(8.5149e-09)\n",
      "episode: 43, total reward: 14.0, average_reward: 23.2, length: 13\n",
      "tensor([ 1.1998,  1.1188,  1.0287,  0.9287,  0.8175,  0.6940,  0.5568,  0.4043,\n",
      "         0.2348,  0.0466, -0.1626, -0.3950, -0.6532, -0.9402, -1.2590, -1.6132,\n",
      "        -2.0068])\n",
      "tensor(4.9086e-08)\n",
      "episode: 44, total reward: 17.0, average_reward: 22.1, length: 16\n",
      "tensor([ 1.1006,  1.0601,  1.0152,  0.9652,  0.9098,  0.8481,  0.7796,  0.7036,\n",
      "         0.6190,  0.5251,  0.4207,  0.3047,  0.1758,  0.0326, -0.1265, -0.3032,\n",
      "        -0.4997, -0.7179, -0.9604, -1.2298, -1.5292, -1.8618, -2.2314])\n",
      "tensor(-2.7211e-07)\n",
      "episode: 45, total reward: 23.0, average_reward: 20.0, length: 22\n",
      "tensor([ 0.9589,  0.9435,  0.9264,  0.9073,  0.8862,  0.8627,  0.8366,  0.8075,\n",
      "         0.7753,  0.7395,  0.6997,  0.6555,  0.6063,  0.5517,  0.4911,  0.4237,\n",
      "         0.3488,  0.2655,  0.1731,  0.0704, -0.0438, -0.1706, -0.3116, -0.4681,\n",
      "        -0.6421, -0.8354, -1.0502, -1.2889, -1.5541, -1.8487, -2.1761, -2.5398])\n",
      "tensor(1.5274e-07)\n",
      "episode: 46, total reward: 32.0, average_reward: 19.9, length: 31\n",
      "tensor([ 0.8901,  0.8809,  0.8707,  0.8593,  0.8467,  0.8327,  0.8171,  0.7998,\n",
      "         0.7806,  0.7592,  0.7355,  0.7091,  0.6798,  0.6472,  0.6111,  0.5709,\n",
      "         0.5262,  0.4766,  0.4214,  0.3601,  0.2921,  0.2164,  0.1324,  0.0390,\n",
      "        -0.0648, -0.1801, -0.3082, -0.4506, -0.6087, -0.7845, -0.9797, -1.1967,\n",
      "        -1.4377, -1.7056, -2.0032, -2.3339, -2.7013])\n",
      "tensor(-6.2827e-08)\n",
      "episode: 47, total reward: 37.0, average_reward: 22.1, length: 36\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 48, total reward: 19.0, average_reward: 23.1, length: 18\n",
      "tensor([ 0.8526,  0.8458,  0.8383,  0.8299,  0.8206,  0.8103,  0.7989,  0.7861,\n",
      "         0.7720,  0.7563,  0.7388,  0.7194,  0.6978,  0.6739,  0.6472,  0.6176,\n",
      "         0.5848,  0.5483,  0.5077,  0.4626,  0.4125,  0.3568,  0.2950,  0.2262,\n",
      "         0.1499,  0.0650, -0.0292, -0.1340, -0.2504, -0.3797, -0.5234, -0.6830,\n",
      "        -0.8604, -1.0575, -1.2765, -1.5198, -1.7902, -2.0906, -2.4244, -2.7953])\n",
      "tensor(-2.2650e-07)\n",
      "episode: 49, total reward: 40.0, average_reward: 24.2, length: 39\n",
      "tensor([ 1.2570,  1.1216,  0.9711,  0.8039,  0.6181,  0.4117,  0.1824, -0.0724,\n",
      "        -0.3556, -0.6702, -1.0197, -1.4081, -1.8397])\n",
      "tensor(3.0204e-07)\n",
      "episode: 50, total reward: 13.0, average_reward: 23.7, length: 12\n",
      "tensor([ 1.1998,  1.1188,  1.0287,  0.9287,  0.8175,  0.6940,  0.5568,  0.4043,\n",
      "         0.2348,  0.0466, -0.1626, -0.3950, -0.6532, -0.9402, -1.2590, -1.6132,\n",
      "        -2.0068])\n",
      "tensor(4.9086e-08)\n",
      "episode: 51, total reward: 17.0, average_reward: 24.4, length: 16\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 52, total reward: 19.0, average_reward: 23.1, length: 18\n",
      "tensor([ 1.2680,  1.1124,  0.9395,  0.7475,  0.5340,  0.2969,  0.0334, -0.2594,\n",
      "        -0.5847, -0.9461, -1.3477, -1.7939])\n",
      "tensor(1.3411e-07)\n",
      "episode: 53, total reward: 12.0, average_reward: 22.9, length: 11\n",
      "tensor([ 1.2680,  1.1124,  0.9395,  0.7475,  0.5340,  0.2969,  0.0334, -0.2594,\n",
      "        -0.5847, -0.9461, -1.3477, -1.7939])\n",
      "tensor(1.3411e-07)\n",
      "episode: 54, total reward: 12.0, average_reward: 22.4, length: 11\n",
      "tensor([ 1.2154,  1.1237,  1.0218,  0.9087,  0.7829,  0.6432,  0.4879,  0.3154,\n",
      "         0.1238, -0.0892, -0.3258, -0.5887, -0.8809, -1.2054, -1.5661, -1.9668])\n",
      "tensor(1.1176e-07)\n",
      "episode: 55, total reward: 16.0, average_reward: 21.7, length: 15\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 56, total reward: 15.0, average_reward: 20.0, length: 14\n",
      "tensor([ 1.0194,  0.9959,  0.9698,  0.9408,  0.9086,  0.8727,  0.8330,  0.7887,\n",
      "         0.7396,  0.6850,  0.6244,  0.5570,  0.4821,  0.3989,  0.3065,  0.2037,\n",
      "         0.0896, -0.0372, -0.1781, -0.3346, -0.5086, -0.7019, -0.9166, -1.1552,\n",
      "        -1.4204, -1.7149, -2.0423, -2.4059])\n",
      "tensor(1.8733e-07)\n",
      "episode: 57, total reward: 28.0, average_reward: 19.1, length: 27\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 58, total reward: 19.0, average_reward: 19.1, length: 18\n",
      "tensor([ 1.2825,  1.0733,  0.8409,  0.5826,  0.2956, -0.0232, -0.3775, -0.7711,\n",
      "        -1.2085, -1.6945])\n",
      "tensor(2.9802e-08)\n",
      "episode: 59, total reward: 10.0, average_reward: 16.1, length: 9\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 60, total reward: 15.0, average_reward: 16.3, length: 14\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 61, total reward: 19.0, average_reward: 16.5, length: 18\n",
      "tensor([ 1.1172,  1.0720,  1.0218,  0.9659,  0.9039,  0.8350,  0.7584,  0.6733,\n",
      "         0.5788,  0.4737,  0.3570,  0.2273,  0.0832, -0.0769, -0.2548, -0.4525,\n",
      "        -0.6722, -0.9162, -1.1874, -1.4887, -1.8234, -2.1954])\n",
      "tensor(2.2623e-07)\n",
      "episode: 62, total reward: 22.0, average_reward: 16.8, length: 21\n",
      "tensor([ 1.1172,  1.0720,  1.0218,  0.9659,  0.9039,  0.8350,  0.7584,  0.6733,\n",
      "         0.5788,  0.4737,  0.3570,  0.2273,  0.0832, -0.0769, -0.2548, -0.4525,\n",
      "        -0.6722, -0.9162, -1.1874, -1.4887, -1.8234, -2.1954])\n",
      "tensor(2.2623e-07)\n",
      "episode: 63, total reward: 22.0, average_reward: 17.8, length: 21\n",
      "tensor([ 0.8291,  0.8236,  0.8174,  0.8106,  0.8030,  0.7946,  0.7853,  0.7749,\n",
      "         0.7633,  0.7505,  0.7363,  0.7204,  0.7028,  0.6833,  0.6616,  0.6374,\n",
      "         0.6106,  0.5808,  0.5477,  0.5109,  0.4700,  0.4246,  0.3742,  0.3181,\n",
      "         0.2558,  0.1866,  0.1097,  0.0242, -0.0708, -0.1763, -0.2935, -0.4237,\n",
      "        -0.5685, -0.7293, -0.9079, -1.1065, -1.3270, -1.5721, -1.8445, -2.1470,\n",
      "        -2.4832, -2.8568])\n",
      "tensor(-1.4192e-09)\n",
      "episode: 64, total reward: 42.0, average_reward: 20.8, length: 41\n",
      "tensor([ 1.0352e+00,  1.0091e+00,  9.8002e-01,  9.4773e-01,  9.1187e-01,\n",
      "         8.7201e-01,  8.2773e-01,  7.7853e-01,  7.2386e-01,  6.6312e-01,\n",
      "         5.9563e-01,  5.2064e-01,  4.3731e-01,  3.4473e-01,  2.4186e-01,\n",
      "         1.2756e-01,  5.6426e-04, -1.4054e-01, -2.9733e-01, -4.7154e-01,\n",
      "        -6.6511e-01, -8.8018e-01, -1.1191e+00, -1.3847e+00, -1.6797e+00,\n",
      "        -2.0075e+00, -2.3717e+00])\n",
      "tensor(-3.2231e-07)\n",
      "episode: 65, total reward: 27.0, average_reward: 21.9, length: 26\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 66, total reward: 15.0, average_reward: 21.9, length: 14\n",
      "tensor([ 1.0039,  0.9827,  0.9593,  0.9332,  0.9042,  0.8720,  0.8362,  0.7964,\n",
      "         0.7522,  0.7032,  0.6486,  0.5880,  0.5207,  0.4459,  0.3627,  0.2704,\n",
      "         0.1677,  0.0537, -0.0730, -0.2138, -0.3702, -0.5440, -0.7372, -0.9517,\n",
      "        -1.1901, -1.4551, -1.7494, -2.0765, -2.4398])\n",
      "tensor(5.7549e-08)\n",
      "episode: 67, total reward: 29.0, average_reward: 22.0, length: 28\n",
      "tensor([ 1.1673,  1.1035,  1.0326,  0.9538,  0.8663,  0.7691,  0.6610,  0.5410,\n",
      "         0.4076,  0.2594,  0.0947, -0.0883, -0.2916, -0.5175, -0.7685, -1.0474,\n",
      "        -1.3573, -1.7016, -2.0842])\n",
      "tensor(-5.6468e-08)\n",
      "episode: 68, total reward: 19.0, average_reward: 22.0, length: 18\n",
      "tensor([ 1.2154,  1.1237,  1.0218,  0.9087,  0.7829,  0.6432,  0.4879,  0.3154,\n",
      "         0.1238, -0.0892, -0.3258, -0.5887, -0.8809, -1.2054, -1.5661, -1.9668])\n",
      "tensor(1.1176e-07)\n",
      "episode: 69, total reward: 16.0, average_reward: 22.6, length: 15\n",
      "tensor([ 1.1837,  1.1119,  1.0321,  0.9434,  0.8449,  0.7354,  0.6138,  0.4786,\n",
      "         0.3285,  0.1616, -0.0237, -0.2297, -0.4586, -0.7129, -0.9955, -1.3094,\n",
      "        -1.6583, -2.0459])\n",
      "tensor(2.7484e-07)\n",
      "episode: 70, total reward: 18.0, average_reward: 22.9, length: 17\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 71, total reward: 15.0, average_reward: 22.5, length: 14\n",
      "tensor([ 1.1507,  1.0939,  1.0308,  0.9607,  0.8828,  0.7962,  0.7001,  0.5932,\n",
      "         0.4745,  0.3425,  0.1960,  0.0331, -0.1479, -0.3490, -0.5724, -0.8206,\n",
      "        -1.0964, -1.4029, -1.7434, -2.1218])\n",
      "tensor(1.7881e-08)\n",
      "episode: 72, total reward: 20.0, average_reward: 22.3, length: 19\n",
      "tensor([ 1.2303,  1.1263,  1.0107,  0.8823,  0.7396,  0.5810,  0.4049,  0.2092,\n",
      "        -0.0083, -0.2500, -0.5185, -0.8168, -1.1483, -1.5166, -1.9258])\n",
      "tensor(1.9471e-07)\n",
      "episode: 73, total reward: 15.0, average_reward: 21.6, length: 14\n",
      "tensor([ 1.1340,  1.0833,  1.0271,  0.9646,  0.8951,  0.8179,  0.7322,  0.6369,\n",
      "         0.5310,  0.4134,  0.2827,  0.1374, -0.0239, -0.2032, -0.4024, -0.6238,\n",
      "        -0.8697, -1.1430, -1.4466, -1.7840, -2.1588])\n",
      "tensor(-1.8023e-07)\n",
      "episode: 74, total reward: 21.0, average_reward: 19.5, length: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn [49], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m rewards \u001b[39m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m steps \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n\u001b[1;32m     18\u001b[0m     \u001b[39m#img.set_data(env.render())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#display.display(plt.gcf())\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#display.clear_output(wait=True)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     action, log_prob \u001b[39m=\u001b[39m policy_net\u001b[39m.\u001b[39;49mget_action(state)\n\u001b[1;32m     23\u001b[0m     new_state, reward, done, _, _\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     24\u001b[0m     log_probs\u001b[39m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[0;32mIn [43], line 24\u001b[0m, in \u001b[0;36mPolicyNetwork.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(Variable(state))\n\u001b[0;32m---> 24\u001b[0m highest_prob_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_actions, p\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49msqueeze(probs\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()))\n\u001b[1;32m     25\u001b[0m log_prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)[highest_prob_action])\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m highest_prob_action, log_prob\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb4d79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model2(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68ea568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mnamed_parameters(\u001b[39mself\u001b[39;49m):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "for i in model.named_parameters(self):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6b7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.distributions.categorical.Categorical(torch.tensor([0.3, 0.7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38217204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb647c27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7b5811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.2040)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.log_prob(m.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ee69aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7969f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6769f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "918c4f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8112, -0.0583, -0.1627],\n",
       "        [-0.0245,  1.2620,  0.9236]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf7484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_agnostic_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:19:13) [Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "da70d7e5fe27d33539e3482db35f628ac7f39ba246fcb0b66647a909a479441e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
